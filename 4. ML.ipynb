{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26575807-15c7-48aa-918c-bf85e395daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,csv\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import (make_pipeline, Pipeline)\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, GridSearchCV)\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f9362-2711-4194-b138-68eedd46292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install thulac\n",
    "import thulac\n",
    "thu = thulac.thulac(T2S=True, seg_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3301c1-9879-451b-8136-6291c63a63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_configuration():\n",
    "    Vectorizers = [CountVectorizer, TfidfVectorizer]\n",
    "    Classifiers = [#MultinomialNB(), \n",
    "                   #LogisticRegression(max_iter=1000),\n",
    "                   SVC(kernel='rbf'), \n",
    "                   SVC(kernel='linear')\n",
    "                  ]\n",
    "    config = [Vectorizers, Classifiers]\n",
    "    configurations = list(itertools.product(*config))\n",
    "    return configurations\n",
    "\n",
    "def down_sample_majority(df, majortopic, downsample):\n",
    "        majority = int(len(df[df[majortopic]==0])/len(df)<0.5) # when the ratio of label=0 < .5, majority = 1, else majority = 0\n",
    "        monority = 1 - majority # if majority = 1 then minority = 0, and vice versa\n",
    "        df_majority = df[df[majortopic]==majority]\n",
    "        df_minority = df[df[majortopic]==monority]\n",
    "        n = int(len(df_minority))*downsample\n",
    "        if n > len(df_majority):\n",
    "            n = len(df_majority)\n",
    "        df_majority_downsampled = resample(df_majority,\n",
    "                                         replace=False,     #\n",
    "                                         n_samples=n # set to N of minority topic\n",
    "                                        # random_state=123) #\n",
    "                                        )\n",
    "        df_downsampled = pd.concat([df_minority, df_majority_downsampled])\n",
    "        print(len(df_minority))\n",
    "        return df_downsampled\n",
    "    \n",
    "def machine_learning(df, labels, downsample = 0):\n",
    "    acc = pd.DataFrame(columns = ['Vectorizer', 'Classifier','Parameters', 'F1','Precision','Recall','Accuracy','Ratio'])\n",
    "\n",
    "    if downsample > 0:\n",
    "        df = down_sample_majority(df, labels, downsample)\n",
    "\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(df['seg'].to_list(), df[labels].to_list(), test_size=0.2)\n",
    "\n",
    "    # scorer = make_scorer(metrics.precision_score, pos_label=1, zero_division=0)\n",
    "    # if len(Counter(train_labels)) > 2:\n",
    "    #     scorer = 'accuracy'\n",
    "\n",
    "    scorer = 'accuracy'\n",
    "    \n",
    "    print(f'train: {len(train_labels)}, test: {len(test_labels)}')\n",
    "    print(Counter(train_labels))\n",
    "    \n",
    "    CV = CountVectorizer()\n",
    "    TV = TfidfVectorizer()\n",
    "\n",
    "    \n",
    "    configurations = combine_configuration()\n",
    "    acc_max = 0\n",
    "    for vectorizer, classifier in configurations:\n",
    "        pipeline = Pipeline(steps = [\n",
    "          (\"vectorizer\", vectorizer()), \n",
    "          (\"classifier\", classifier)])\n",
    "\n",
    "        grid = {\"vectorizer__ngram_range\": [(1,1), (1,2)],\n",
    "                \"vectorizer__max_df\": [0.5, 1.0],\n",
    "                \"vectorizer__min_df\": [0, 5],\n",
    "                \"classifier__C\": [0.01, 1, 100]\n",
    "               }\n",
    "        print(vectorizer, classifier)\n",
    "        try:\n",
    "            search=GridSearchCV(estimator=pipeline, n_jobs=-1, param_grid=grid, scoring=scorer, cv=5)\n",
    "            search.fit(train_texts, train_labels)\n",
    "        except:\n",
    "            #print('regularization is not applicable')\n",
    "            grid.pop('classifier__C')\n",
    "            search=GridSearchCV(estimator=pipeline, n_jobs=-1, param_grid=grid, scoring=scorer, cv=5)\n",
    "            search.fit(train_texts, train_labels)\n",
    "        y_pred = search.predict(test_texts)\n",
    "        # print(y_pred.mean())\n",
    "        # y_pred = search.predict(train_texts)\n",
    "        acc = acc.append({'Vectorizer':vectorizer, 'Classifier':classifier,'Parameters':search.best_params_, \n",
    "                          #   'Precision':metrics.precision_score(test_labels,y_pred),\n",
    "                          #   'Recall':metrics.recall_score(test_labels,y_pred),\n",
    "                          # 'F1':metrics.f1_score(test_labels,y_pred),\n",
    "                                    'Accuracy':metrics.accuracy_score(test_labels,y_pred),\n",
    "                                    'Ratio':Counter(train_labels)[1]/(len(train_labels))},ignore_index=True)\n",
    "        if metrics.accuracy_score(test_labels,y_pred) > acc_max:\n",
    "            search_max = search\n",
    "    return acc, search_max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144dda7e-d62c-4a5c-984c-3d3a0d13a631",
   "metadata": {},
   "source": [
    "segmentation for Weibo was done and saved befored by also using \".apply(lambda x: thu.cut(x, text=True))\" \n",
    "\n",
    "I don't include it here because it costs hours to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a7029-b255-4b19-9eb5-ddff875ff5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_posts = pd.read_csv('data/weibo_posts_moral_seg.csv')\n",
    "weibo_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838e24c-bf2d-4349-a17d-caad6c110c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhihu_posts = pd.read_csv('data/zhihu_answers_moral.csv')\n",
    "zhihu_posts['seg'] = zhihu_posts['post_content'].apply(lambda x: thu.cut(x, text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a48a3-289e-40a4-8202-67e1bc7d282c",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce228e-6e6c-440f-8c98-7f95f2b79fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('annotation.xlsx')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e38e9-aadf-44fa-8723-5367740327e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['seg'] = data['post_content'].apply(lambda x: thu.cut(x, text=True))\n",
    "data['pure_fem'] = data['Stance_post'].apply(lambda x: 1 if x == 1 else 0)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c472f0a3-8689-4b0d-b98b-0a40fcd4f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Relevant'\n",
    "acc, search = machine_learning(train_data,label,downsample = 2)\n",
    "pred = search.predict(test_data['seg'])\n",
    "print(metrics.classification_report(test_data[label],pred))\n",
    "# weibo_posts['pred_relevant'] = search.predict(weibo_posts['seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350514e-f4f0-4bbc-b500-800dce89d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhihu_posts['pred_relevant'] = search.predict(zhihu_posts['seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03681f2b-9a4c-4eae-a821-5c648f9ed6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Stance_post'\n",
    "acc, search_stance = machine_learning(train_data,label,downsample = 0)\n",
    "pred_stance = search_stance.predict(test_data['seg'])\n",
    "print(metrics.classification_report(test_data[label],pred_stance))\n",
    "# weibo_posts['pred_stance'] = search_stance.predict(weibo_posts['seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78a0c6-b62b-47bd-8332-99b1124ce28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhihu_posts['pred_stance'] = search_stance.predict(zhihu_posts['seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a45d6-8164-470b-bb9f-399f1f4909a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Mention_feminist'\n",
    "acc, search_mfem = machine_learning(train_data,label,downsample = 1)\n",
    "pred_mfem = search_mfem.predict(test_data['seg'])\n",
    "print(metrics.classification_report(test_data[label],pred_mfem))\n",
    "# weibo_posts['pred_mfem'] = search_mfem.predict(weibo_posts['seg'])\n",
    "zhihu_posts['pred_mfem'] = search_mfem.predict(zhihu_posts['seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c316f5-9bba-4ad4-97a8-5038d5a95ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Mention_antifeminist'\n",
    "acc, search_matfem = machine_learning(train_data,label,downsample = 1)\n",
    "pred_matfem = search_matfem.predict(test_data['seg'])\n",
    "print(metrics.classification_report(test_data[label],pred_matfem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d6d5c6-a7b8-483c-a965-e54e3cb50622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weibo_posts['pred_matfem'] = search_matfem.predict(weibo_posts['seg'])\n",
    "zhihu_posts['pred_matfem'] = search_matfem.predict(zhihu_posts['seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a16b76-9d32-4a5b-9528-c31a1d4bd9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Engagement'\n",
    "acc, search_egm = machine_learning(train_data,label,downsample = 1)\n",
    "pred_egm = search_egm.predict(test_data['seg'])\n",
    "print(metrics.classification_report(test_data[label],pred_egm))\n",
    "# weibo_posts['pred_engagement'] = search_egm.predict(weibo_posts['seg'])\n",
    "zhihu_posts['pred_engagement'] = search_egm.predict(zhihu_posts['seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d72ec-dd29-41c7-8ee5-ca1a84b57ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_posts.to_csv('weibo_posts_ML.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f0e24-5e60-4a1e-bc7a-08442dab3eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhihu_posts.to_csv('zhihu_posts_ML.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfbd87-610d-4e1f-85b4-dc9673c5b3b6",
   "metadata": {},
   "source": [
    "## Machine learning for comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af333c8-7885-4f5a-b8a1-0a49719a7275",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = data[(data['Is_comment']==1) & (data['Relevant']==1)]\n",
    "stance_dict = {0:'无立场',1:'明确 的 女权主义者',2:'女权主义者 但 对 其他 女权主义者 表示 不满',3:'反 女权主义者'}\n",
    "for i, comment in comments.iterrows():\n",
    "    comments.loc[i,'content'] = f\"被评论的文本为：{comment['post_content']}。被评论者的立场为：{stance_dict[comment['Stance_post']]}。评论内容为：{comment['comment_content']}\"\n",
    "comments['seg'] = comments['content'].apply(lambda x: thu.cut(x, text=True))\n",
    "train_data_cm, test_data_cm = train_test_split(comments, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8677a75-f505-4253-9edf-b93d68f287da",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Stance_comment'\n",
    "acc, search_stance_cm = machine_learning(train_data_cm,label,downsample = 0)\n",
    "pred_stance_cm = search_stance_cm.predict(test_data_cm['seg'])\n",
    "print(metrics.classification_report(test_data_cm[label],pred_stance_cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c8f81-648a-4e04-b2f2-6fa6473b5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_comments['pred_stance_comment'] = search_stance_cm.predict(weibo_comments['seg_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0569b719-e55a-4bf0-ba78-33d2ce0f4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_comments.drop('seg_content', axis=1).to_csv('weibo_comments_ML.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f082eb-02b9-43f0-9419-3e9b277f46b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comments segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94c86d-4cf4-4c2f-8020-12de4a90f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e079cee-d60f-4706-a941-251c24cc955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_comments = pd.read_csv('data/weibo_comments_moral_seg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055573a-026c-4dbc-b124-e0ff7f9197f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_posts = pd.read_csv('weibo_posts_ML.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf4a7b-401a-4cc1-ad77-6fe889425827",
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_dict = {0:'无立场',1:'明确 的 女权主义者',2:'女权主义者 但 对 其他 女权主义者 表示 不满',3:'反 女权主义者'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba23fa-58d9-48fd-ad49-f2cf88f04e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, comment in tqdm(weibo_comments.iterrows()):\n",
    "    match = weibo_posts[weibo_posts['mid']==comment['answer_id']]\n",
    "    try:\n",
    "        relevant = match['pred_relevant'].values[0]\n",
    "        post_stance = match['pred_stance'].values[0]\n",
    "        strance_str = stance_dict[post_stance]\n",
    "    except:\n",
    "        continue\n",
    "    if comment['pred_relevant'] == relevant:\n",
    "        continue\n",
    "    weibo_comments.loc[i,'pred_relevant'] = relevant\n",
    "    weibo_comments.loc[i,'pred_stance_post'] = post_stance\n",
    "    # seg_comment = thu.cut(comment['comment_content'], text=True)\n",
    "    seg_comment = re.sub(r\"被 评论者 的 立场 为 ： .* 。 评论 内容 为 ： \",f\"被 评论者 的 立场 为 ： {strance_str} 。 评论 内容 为 ： \", comment['seg_content'])\n",
    "    weibo_comments.loc[i,'seg_content'] = seg_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b6a4dc-f4d4-4339-b66b-8827073bd97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_comments = weibo_comments[~weibo_comments['pred_relevant'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f84d06-65b3-4ee2-a137-37762f199bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weibo_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aef774e-faee-4633-9dad-c908a2d763ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_comments.to_csv('weibo_comments_moral_seg.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad205d41-10dc-482d-8e42-0ab937227d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhihu_comments = pd.read_csv('data/zhihu_comments_moral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72160a14-3728-44c2-8b33-6d70d85b2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, comment in tqdm(zhihu_comments.iterrows()):\n",
    "    match = zhihu_posts[zhihu_posts['answer_id']==comment['answer_id']]\n",
    "    try:\n",
    "        relevant = match['pred_relevant'].values[0]\n",
    "        post_stance = match['pred_stance'].values[0]\n",
    "    except:\n",
    "        continue\n",
    "    zhihu_comments.loc[i,'pred_relevant'] = relevant\n",
    "    zhihu_comments.loc[i,'pred_stance_post'] = post_stance\n",
    "    text = f\"被 评论 的 文本 为 ： {comment['post_content']} 。 被 评论者 的 立场 为 ： {stance_dict[post_stance]} 。 评论 内容 为 ： {comment['comment_content']}\"\n",
    "    seg_comment = thu.cut(text, text=True)\n",
    "    zhihu_comments.loc[i,'seg_content'] = seg_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a45d9-a22c-49c4-82f8-e9b385ddcbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhihu_comments = zhihu_comments[~zhihu_comments['seg_content'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f0c75-8aee-4446-b2cc-bbe5f1514bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhihu_comments['pred_stance_comment'] = search_stance_cm.predict(zhihu_comments['seg_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5174f0-3a70-493f-8634-060ea5ea9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhihu_comments.to_csv('zhihu_comments_ML.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
